import hashlib
import os
import tarfile
import zipfile
import requests
import numpy as np
import pandas as pd
import torch
from torch import nn
from d2l import torch as d2l

from 权重衰退 import n_train, batch_size, num_epochs

#@save
DATA_HUB = dict()
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'

def download(name, cache_dir=os.path.join('..', 'data')):  #@save
    """下载一个DATA_HUB中的文件，返回本地文件名"""
    assert name in DATA_HUB, f"{name} 不存在于 {DATA_HUB}"
    url, sha1_hash = DATA_HUB[name]
    os.makedirs(cache_dir, exist_ok=True)
    fname = os.path.join(cache_dir, url.split('/')[-1])
    if os.path.exists(fname):
        sha1 = hashlib.sha1()
        with open(fname, 'rb') as f:
            while True:
                data = f.read(1048576)
                if not data:
                    break
                sha1.update(data)
        if sha1.hexdigest() == sha1_hash:
            return fname  # 命中缓存
    print(f'正在从{url}下载{fname}...')
    r = requests.get(url, stream=True, verify=True)
    with open(fname, 'wb') as f:
        f.write(r.content)
    return fname

def download_extract(name, folder=None):  #@save
    """下载并解压zip/tar文件"""
    fname = download(name)
    base_dir = os.path.dirname(fname)
    data_dir, ext = os.path.splitext(fname)
    if ext == '.zip':
        fp = zipfile.ZipFile(fname, 'r')
    elif ext in ('.tar', '.gz'):
        fp = tarfile.open(fname, 'r')
    else:
        assert False, '只有zip/tar文件可以被解压缩'
    fp.extractall(base_dir)
    return os.path.join(base_dir, folder) if folder else data_dir

def download_all():  #@save
    """下载DATA_HUB中的所有文件"""
    for name in DATA_HUB:
        download(name)

DATA_HUB['kaggle_house_train'] = (  #@save
    DATA_URL + 'kaggle_house_pred_train.csv',
    '585e9cc93e70b39160e7921475f9bcd7d31219ce')

DATA_HUB['kaggle_house_test'] = (  #@save
    DATA_URL + 'kaggle_house_pred_test.csv',
    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')

train_data = pd.read_csv(download('kaggle_house_train'))
test_data = pd.read_csv(download('kaggle_house_test'))

print(train_data.shape)
print(test_data.shape)
print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])

k=5
num_epochs=200
lr=0.1
wd=50000
batch_size=128

all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))

numeric_feature = all_features.dtypes[all_features.dtypes != 'object'].index
all_features[numeric_feature] = all_features[numeric_feature].apply(lambda x:(x-x.mean())/(x.std()))
all_features[numeric_feature] = all_features[numeric_feature].fillna(0)

all_features = pd.get_dummies(all_features,dummy_na=True)
all_features=all_features*1
print(all_features.shape)

n_train = train_data.shape[0]
train_features = torch.tensor(all_features[:n_train].values,dtype=torch.float32)
test_features = torch.tensor(all_features[n_train:].values,dtype=torch.float32)
train_labels = torch.tensor(train_data.SalePrice.values.reshape(-1,1),dtype=torch.float32)

loss=nn.MSELoss()
in_features=train_features.shape[1]

def log_rmse(net,features,labels):
    clipped_preds=torch.clamp(net(features),1,float('inf'))
    rmse=torch.sqrt(loss(torch.log(clipped_preds),torch.log(labels)))
    return rmse.item()

def train(net,train_features,train_labels,test_features,test_labels):
    net.train()
    updater=torch.optim.Adam(net.parameters(),lr=lr,weight_decay=wd)
    train_iter=d2l.load_array((train_features,train_labels),batch_size)
    test_iter = d2l.load_array((test_features,test_labels), batch_size)
    train_ls,test_ls = [],[]
    for epoch in range(num_epochs):
        for X,y in train_iter:
            l=loss(net(X),y)
            updater.zero_grad()
            l.backward()
            updater.step()
        # print("train")
        # print(log_rmse(net,train_features,train_labels))
        # if test_labels is not None:
        #     print("test")
        #     print(log_rmse(net,test_features,test_labels))
    return log_rmse(net,train_features,train_labels),log_rmse(net,test_features,test_labels)


def get_k_fold_data(k,i,X,y):
    assert k>1
    fold_size = X.shape[0]//k
    X_train,y_train = None,None
    for j in range(k):
        idx = slice(j*fold_size,(j+1)*fold_size)
        X_part,y_part = X[idx,:],y[idx]
        if j==i:
            X_valid,y_valid = X_part,y_part
        elif X_train is None:
            X_train,y_train = X_part,y_part
        else:
            X_train = torch.cat([X_train,X_part],0)
            y_train = torch.cat([y_train,y_part],0)
    return X_train,y_train,X_valid,y_valid

def k_fold(k,X_train,y_train):
    train_l_sum,valid_l_sum=0,0
    for i in range(k):
        data = get_k_fold_data(k,i,X_train,y_train)
        net = nn.Sequential(nn.Linear(in_features,64),nn.ReLU(),nn.Dropout(0),
                            nn.Linear(64, 32),nn.ReLU(),nn.Dropout(0),
                            nn.Linear(32,1))
        train_ls,vaild_ls = train(net,*data)
        print("训练精度：",train_ls,"验证精度：",vaild_ls)
        train_l_sum+=train_ls
        valid_l_sum+=vaild_ls
    return train_l_sum/k,valid_l_sum/k

print(k_fold(k,train_features,train_labels))